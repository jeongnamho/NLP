{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. N-gram 언어 모델(N-gram Language Model)\n",
    "- n-gram 언어 모델은 여전히 카운트에 기반한 통계적 접근을 사용하고 있으므로 SLM의 일종 \n",
    "- 다만, 앞서 배운 언어 모델과는 달리 이전에 등장한 모든 단어를 고려하는 것이 아니라 일부 단어만 고려하는 접근 방법을 사용\n",
    "- n-gram에서 n이 갖는 의미 : 몇 개의 단어를 볼 것인지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) 코퍼스에서 카운트하지 못하는 경우의 감소\n",
    "\n",
    "### SLM의 한계\n",
    "- 훈련 코퍼스에 확률을 계산하고 싶은 문장이나 단어가 없을 수 있음\n",
    "- 확률을 계산하고 싶은 문장 길이 $\\uparrow$  $\\Longrightarrow$ 코퍼스에 그 문장이 존재하지 않을 가능성$\\uparrow$ (카운트할 수 없을 가능성 $\\uparrow$)\n",
    "\n",
    "### 대안\n",
    "\\begin{align}\n",
    "P(\\textrm{is|An adorable little boy}) \\approx P(\\textrm{is|boy})\n",
    "\\end{align}\n",
    "- 위처럼 An adorable little boy 대신 boy가 나왔을 때 is가 나올 확률로 생각해보기\n",
    "    - 왜냐면 boy is라는 더 짧은 단어 시퀀스가 존재할 가능성이 높아서\n",
    "\n",
    "\\begin{align}\n",
    "P(\\textrm{is|An adorable little boy}) \\approx P(\\textrm{is|little boy})\n",
    "\\end{align}\n",
    "- boy가 짧아서 좀 그러면 little boy로 조금 길이를 늘리는 것도 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) N-gram\n",
    "> $n$ 개의 연속적인 단어 나열을 의미\n",
    "\n",
    "- 갖고 있는 코퍼스에서 n개의 단어 뭉치 단위로 끊어서 이를 하나의 토큰으로 간주"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exmaple : An adorable little boy is spreading smiles\n",
    "- **unigrams** : an, adorable, little, boy, is, spreading, smiles\n",
    "- **bigrams** : an adorable, adorable little, little boy, boy is, is spreading, spreading smiles\n",
    "- **trigrams** : an adorable little, adorable little boy, little boy is, boy is spreading, is spreading smiles\n",
    "- **4-grams** : an adorable little boy, adorable little boy is, little boy is spreading, boy is spreading smiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-gram을 통한 언어 모델에서는 다음에 나올 단어의 예측은 오직 n-1개의 단어에만 의존합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exmaple : An adorable little boy is spreading \\__?\\__\n",
    "- n = 4:\n",
    "<img src=n-gram.png width=300>\n",
    "\n",
    "- Assumption\n",
    "    - boy is spreading가 1,000번 등장\n",
    "    - boy is spreading insults가 500번 등장\n",
    "    - boy is spreading smiles가 200번 등장\n",
    "\n",
    "- Probability\n",
    "\\begin{align}\n",
    "P(\\textrm{insults|boy is spreading}) = 0.500\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "P(\\textrm{smiles|boy is spreading}) = 0.200\n",
    "\\end{align}\n",
    "    - **<ins>확률적 선택에 따라 우리는 insults가 더 맞다고 판단</ins>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) N-gram Language Model의 한계\n",
    "\n",
    "- 앞서 4-gram을 통한 언어 모델 동작 방식에서 **'작고 사랑스러운(an adorable little)'**이라는 수식어를 제거하고 반영X\n",
    "- 만약 an adorable little이 반영되었다면 **'모욕을 퍼뜨렸다'**라는 부정적인 내용이 **'웃음 지었다'**라는 긍정적인 내용 대신 채택되었을까?\n",
    "- 다시 말해, 전체 문장을 고려한 언어 모델보다는 정확도가 떨어질 수밖에 없음\n",
    "\n",
    "### (1) 희소 문제(Sparsity Problem)\n",
    "문장에 존재하는 앞에 나온 단어를 모두 보는 것보다 일부 단어만을 보는 것으로 현실적으로 코퍼스에서 카운트 할 수 있는 확률을 높일 수는 있었지만, n-gram 언어 모델도 여전히 n-gram에 대한 희소 문제가 존재합니다.\n",
    "\n",
    "### (2) n을 선택하는 것은 trade-off 문제\n",
    "n을 작게 선택하면 훈련 코퍼스에서 카운트는 잘 되겠지만 근사의 정확도는 현실의 확률분포와 멀어집니다. 그렇기 때문에 적절한 n을 선택해야 합니다. 앞서 언급한 trade-off 문제로 인해 **정확도를 높이려면 n은 최대 5를 넘게 잡아서는 안 된다고 권장되고 있습니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) 적용 분야(Domain)에 맞는 코퍼스의 수집\n",
    "- 어떤 분야인지, 어떤 어플리케이션인지에 따라서 특정 단어들의 확률 분포는 다름\n",
    "- 가령, 마케팅 분야에서는 마케팅 단어가 빈번하게 등장할 것이고, 의료 분야에서는 의료 관련 단어가 당연히 빈번하게 등장할 것\n",
    "- 이 경우 언어 모델에 사용하는 코퍼스를 해당 도메인의 코퍼스를 사용한다면 당연히 언어 모델이 제대로 된 언어 생성을 할 가능성이 높아짐\n",
    "- 때로는 이를 언어 모델의 약점이라고 하는 경우도 있는데, 훈련에 사용된 도메인 코퍼스가 무엇이냐에 따라서 성능이 비약적으로 달라지기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) 인공 신경망을 이용한 언어 모델(Neural Network Based Language Model)\n",
    "- Ch.8 에서 다룰 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
