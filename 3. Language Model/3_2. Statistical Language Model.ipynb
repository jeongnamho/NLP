{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 통계적 언어 모델(Statistical Language Model, SLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) 조건부 확률"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "P(A \\cap B) = P(A,B)\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "P(A,B)=P(A)P(B|A)\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "P(A,B,C,D)=P(A)P(B|A)P(C|A,B)P(D|A,B,C)\n",
    "\\end{align}\n",
    "\n",
    "- 연쇄 법칙(chain rule)\n",
    "\\begin{align}\n",
    "P(x_1,x_2,x_3...x_n)=P(x_1)P(x_2|x_1)P(x_3|x_1,x_2)...P(x_n|x_1...x_{n−1})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 문장에 대한 확률\n",
    "- P(An adorable little boy is spreading smiles)를 식으로 표현\n",
    "\n",
    "\\begin{align}\n",
    "P(\\textrm{An adorable little boy is spreading smiles})=\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "P(\\textrm{An})×P(\\textrm{adorable|An})×P(\\textrm{little|An adorable})×P(\\textrm{boy|An adorable little})×P(\\textrm{is|An adorable little boy}) ×P(\\textrm{spreading|An adorable little boy is})×P(\\textrm{smiles|An adorable little boy is spreading})\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) 카운트 기반의 접근\n",
    "\n",
    "\\begin{align}\n",
    "P(\\textrm{is|An adorable little boy}) = \\frac{count(\\textrm{An adorable little boy is}}{count(\\textrm{An adorable little boy})}\n",
    "\\end{align}\n",
    "\n",
    "예를 들어 기계가 학습한 코퍼스 데이터에서 An adorable little boy가 100번 등장했는데 그 다음에 is가 등장한 경우는 30번이라고 합시다. 이 경우 P(is|An adorable little boy)는 30%입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) 카운트 기반 접근의 한계 - 희소 문제(Sparsity Problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 카운트 기반으로 접근하려고 한다면 갖고있는 코퍼스(corpus). 즉, 다시 말해 <ins>기계가 훈련하는 데이터는 정말 방대한 양이 필요</ins>\n",
    "- 예를 들어, 3)에서 $P(\\textrm{is|An adorable little boy})$를 구할 때, \n",
    "    - count(An adorable little boy is) = 0 $\\Longrightarrow$ $P(\\textrm{is|An adorable little boy})$ = 0\n",
    "    - count(An adorable little boy) = 0 $\\Longrightarrow$ $\\nexists P(\\textrm{is|An adorable little boy})$\n",
    "    \n",
    "    \n",
    "충분한 데이터를 관측하지 못하여 언어를 정확히 모델링하지 못하는 문제를 **희소 문제(sparsity problem)**라고 합니다.\n",
    "\n",
    "위 문제를 완화하는 방법으로 다음 챕터에서 배우는 n-gram이나 이 책에서 다루지는 않지만 스무딩이나 백오프와 같은 여러가지 일반화(generalization) 기법이 존재합니다. 하지만 희소 문제에 대한 근본적인 해결책은 되지 못하였습니다. 결국 이러한 한계로 인해 언어 모델의 트렌드는 통계적 언어 모델에서 인공 신경망 언어 모델로 넘어가게 됩니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
