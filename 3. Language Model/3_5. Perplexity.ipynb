{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 펄플렉서티(Perplexity)\n",
    "\n",
    "두 개의 모델 A, B가 있을 때 이 모델의 성능은 어떻게 비교할 수 있을까요? 두 개의 모델을 오타 교정, 기계 번역 등의 평가에 투입해볼 수 있겠습니다. 그리고 두 모델이 해당 업무의 성능을 누가 더 잘했는지를 비교하면 되겠습니다. 그런데 두 모델의 성능을 비교하고자, 일일히 모델들에 대해서 실제 작업을 시켜보고 정확도를 비교하는 작업은 공수가 너무 많이 드는 작업입니다. 만약 비교해야하는 모델이 두 개가 아니라 그 이상의 수라면 시간은 비교해야하는 모델의 수만큼 배로 늘어날 수도 있습니다.\n",
    "\n",
    "이러한 평가를 외부 평가(extrinsic evaluation)라고 하는데, 이러한 평가보다는 어쩌면 조금은 부정확할 수는 있어도 테스트 데이터에 대해서 빠르게 식으로 계산되는 더 간단한 평가 방법이 있습니다. 바로 모델 내에서 자신의 성능을 수치화하여 결과를 내놓는 내부 평가(Intrinsic evaluation)에 해당되는 펄플렉서티(perplexity)입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) 언어 모델의 평가 방법(Evaluation metric) : PPL\n",
    "- PPL : 단어의 수로 정규화(normalization) 된 테스트 데이터에 대한 확률의 역수\n",
    "- PPL은 수치가 **'낮을수록' 언어 모델의 성능이 좋다는 것을 의미**\n",
    "- 문장 $W$의 길이가 $N$일 때,\n",
    "\\begin{align}\n",
    "PPL(W) = P(w_1,w_2,w_3...w_N)^{-\\frac{1}{N}}= \\sqrt[\\leftroot{5}\\uproot{5}p]{\\tfrac{1}{P(w_1,w_2,w_3...w_N)}} \n",
    "\\end{align}\n",
    "- 문장의 확률에 Chain Rule을 적용하면 아래와 같음\n",
    "\\begin{align}\n",
    "PPL(W) = \\sqrt[\\leftroot{5}\\uproot{5}p]{\\tfrac{1}{P(w_1,w_2,w_3...w_N)}} = \\sqrt[\\leftroot{5}\\uproot{5}p]{\\tfrac{1}{\\displaystyle\\prod_{i=1}^{N}P(w_i|w_1,w_2,...w_{i-1})}}\n",
    "\\end{align}\n",
    "- 여기에 n-gram을 적용할 수도 있음(n=1)\n",
    "\\begin{align}\n",
    "PPL(W) = \\sqrt[\\leftroot{5}\\uproot{5}p]{\\tfrac{1}{\\displaystyle\\prod_{i=1}^{N}P(w_i|w_{i-1})}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 분기 계수(Branching factor)\n",
    "- PPL은 선택할 수 있는 가능한 경우의 수를 의미하는 분기계수(branching factor)\n",
    "- PPL은 이 언어 모델이 특정 시점에서 평균적으로 몇 개의 선택지를 가지고 고민하고 있는지를 의미\n",
    "- Example  \n",
    "언어 모델에 어떤 테스트 데이터을 주고 측정했더니 PPL이 10이 나왔다고 해봅시다. 그렇다면 해당 언어 모델은 테스트 데이터에 대해서 다음 단어를 예측하는 모든 시점(time-step)마다 평균적으로 10개의 단어를 가지고 어떤 것이 정답인지 고민하고 있다고 볼 수 있습니다. 같은 테스트 데이터에 대해서 두 언어 모델의 PPL을 각각 계산 후에 PPL의 값을 비교하면, 두 언어 모델 중 어떤 것이 성능이 좋은지도 판단이 가능합니다. 당연히 PPL이 더 낮은 언어 모델의 성능이 더 좋다고 볼 수 있습니다.\n",
    "\n",
    "\\begin{align}\n",
    "PPL(W) = P(w_1,w_2,w_3...w_N)^{-\\frac{1}{N}} = \\{(\\frac{1}{10})^N\\}^{-\\frac{1}{N}} = \\frac{1}{10}^{-1} = 10\n",
    "\\end{align}\n",
    "\n",
    "### 평가 방법에 있어서 주의할 점\n",
    "- PPL의 값이 낮다는 것은 테스트 데이터 상에서 높은 정확도를 보인다는 것이지, 사람이 직접 느끼기에 좋은 언어 모델이라는 것을 반드시 의미하진 않는다는 점\n",
    "- 언어 모델의 PPL은 테스트 데이터에 의존하므로 두 개 이상의 언어 모델을 비교할 때는 정량적으로 양이 많고, 또한 도메인에 알맞은 동일한 테스트 데이터를 사용해야 신뢰도가 높다는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) 기존 언어 모델 Vs. 인공 신경망을 이용한 언어 모델\n",
    "<img src=ppl.png width=500>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
