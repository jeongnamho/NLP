{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 과적합(Overfitting)을 막는 방법들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) 데이터의 양을 늘리기\n",
    "- 데이터 양이 적을 때는 Augmentation을 쓰기도 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 모델의 복잡도 줄이기\n",
    "- 복잡도는 은닉층의 수나 매개변수의 수 등으로 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) 가중치 규제(Regularization) 적용하기\n",
    "> 모델이 복잡해지지 않도록 모델 복잡도에 벌점(penalty)를 주는 것\n",
    "\n",
    "$cost_{regularized}(\\theta; \\mathbb{X}, \\mathbb{Y}) = cost(\\theta;\\mathbb{X}, \\mathbb{Y} ) + \\lambda R(\\theta)$  \n",
    "위 식은 보통 가중치(W)를 갱신할 때만 적용, bias에는 원래 손실함수를 사용(bias는 하나의 노드에만 국한되어 규제를 하지 않아도 문제 될 것이 없음)  \n",
    "가중치를 갱신할 때, 손실함수의 미분값을 이전 가중치에서 빼서 다음 가중치를 계산한다. 따라서 가중치가 크면 손실 함수가 커지고, 다음 가중치가 크게 감소된다.\n",
    "\n",
    "\n",
    "- L1 규제 : $cost_{regularized}(\\mathbb{W};\\mathbb{X},\\mathbb{Y})$ = $\\sum_{i=1}^n \\big(y_i-\\sum_{j=1}^p x_{ij}w_j  \\big)^2 + \\lambda \\sum_{j=1}^p |w_j|$\n",
    "\\begin{align} \\mathbb{W} & = \\mathbb{W} - \\eta \\nabla L_{regularized} \\newline & = \\mathbb{W} - \\eta(\\nabla L + \\lambda sign(\\mathbb{W})) \\newline & = \\mathbb{W} - \\eta \\nabla L - \\eta \\lambda sign(\\mathbb{W}) \\end{align}\n",
    "\n",
    "\n",
    "- L2 규제 : $cost_{regularized}(\\mathbb{W};\\mathbb{X},\\mathbb{Y})$ = $\\sum_{i=1}^n \\big(y_i-\\sum_{j=1}^p x_{ij}w_j  \\big)^2 + \\lambda \\sum_{j=1}^p |w_j|^2$\n",
    "\\begin{align} \\mathbb{W} & = \\mathbb{W} - \\eta \\nabla L_{regularized} \\newline & = \\mathbb{W} - \\eta(\\nabla L + 2\\lambda \\mathbb{W}) \\newline & = (1-2\\eta \\lambda)\\mathbb{W} - \\eta \\nabla L \\end{align}\n",
    "- $\\lambda$는 규제의 강도를 정하는 하이퍼파라미터\n",
    "- $\\lambda$ 가 크다면 규제를 중요시해서 모델이 훈련 데이터에 대해서 적합한 매개 변수를 찾는 것보다 규제를 위해 추가된 항들을 작게 유지하는 것을 우선한다.(**Underfitting**이 된다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) 드롭아웃(Dropout)\n",
    "> 학습 과정에서 신경망의 일부를 사용하지 않는 방법\n",
    "\n",
    "<img src=드롭아웃.png width=500>\n",
    "\n",
    "- 신경망 학습 시에만 사용, 예측시에는 사용하지 않음\n",
    "- 특정 뉴런이나 특정 조합에 너무 의존적이게 되는 것을 방지\n",
    "- 서로 다른 신경망들을 앙상블하여 사용하는 것 같은 효과 $\\Longrightarrow$ 과적합 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_shape=(max_words,), activation='relu'))\n",
    "model.add(Dropout(0.5)) # 드롭아웃 추가. 비율은 50%\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5)) # 드롭아웃 추가. 비율은 50%\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
