{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 기울기 소실(Gradient Vanishing)과 폭주(Exploding)\n",
    "- **기울기 소실** : DNN에서 역전파 과정에서 입력층으로 갈수록 기울기가 점차적으로 작아지는 현상이 생겨 업데이트가 제대로 안되는 현상\n",
    "- **기울기 폭주** :위와 반대로 입력층으로 갈수록 기울기가 비정상적으로 큰 값이 되면서 발산하는 경우(RNN에서 발생할 수 있음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) ReLU와 ReLU의 변형들\n",
    "\n",
    "- 기울기 소실 완화하는 간단한 방법 : 은닉층의 활성화함수로 ReLU, Leaky ReLU를 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 그래디언트 클리핑(Gradient Clipping)\n",
    "\n",
    "- 기울기 값을 자르는 것을 의미\n",
    "- 기울기 폭주를 막기 위해 임계값을 넘지 않도록 값을 자름(RNN에서 유용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "Adam = optimizers.Adam(lr=0.0001, clipnorm=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) 가중치 초기화(Weight initialization)\n",
    "\n",
    "### (1) Xavier\n",
    "Xavier Initialization 혹은 Glorot Initialization라고도 불리는 초기화 방법은 이전 노드와 다음 노드의 개수에 의존하는 방법이다. Xaiver함수는 비선형함수(ex. **sigmoid, tanh**)에서 효과적인 결과를 보여준다.  Xavier Initialization에는 다음과 같은 두가지 방법이 있다.\n",
    "\n",
    "- **Xavier Normal Initialization**\n",
    "\\begin{align}\n",
    "W \\sim N(0,Var(W))\n",
    "\\end{align}\n",
    "\n",
    "Here, $Var(W) = \\sqrt{\\frac{2}{n_{in}+n_{out}}}$, where $n_{in}$은 이전 layer의 노드 수, $n_{out}$은 다음 layer의 노드수\n",
    "\n",
    "- **Xavier Uniform Initialization** : \n",
    "\\begin{align}\n",
    "W \\sim U(- \\sqrt{\\frac{6}{n_{in}+n_{out}}}, \\sqrt{\\frac{6}{n_{in}+n_{out}}})\n",
    "\\end{align}\n",
    "\n",
    "Here, $Var(W) = \\sqrt{\\frac{2}{n_{in}+n_{out}}}$, where $n_{in}$은 이전 layer(input)의 노드 수, $n_{out}$은 다음 layer(output)의 노드수\n",
    "\n",
    "----\n",
    "**하지만 ReLU함수에서 Xavier Initialization을 사용 시 출력 값이 0으로 수렴하게 되는 현상을 확인 할 수 있다. 따라서 ReLU함수에는 또 다른 초기화 방법을 사용해야 한다.**\n",
    "\n",
    "---\n",
    "### (2) He\n",
    "ReLU를 활성화 함수로 사용 시 Xavier 초기값 설정이 비효율적인 결과를 보이는 것을 확인할 수 있는데, 이런 경우 He initialization을 사용한다. 이 방법 또한 정규분포와 균등분포 두가지 방법이 사용된다.(He et al. ,2015)\n",
    "- **He Normal Initialization**\n",
    "\\begin{align}\n",
    "W \\sim N(0,Var(W))\n",
    "\\end{align}\n",
    "\n",
    "Here, $Var(W) = \\sqrt{\\frac{2}{n_{in}}}$, where $n_{in}$은 이전 layer(input)의 노드 수\n",
    "- **Normal distribution**\n",
    "\\begin{align}\n",
    "W \\sim U(- \\sqrt{\\frac{6}{n_{in}}}, \\sqrt{\\frac{6}{n_{in}}})\n",
    "\\end{align}\n",
    "\n",
    "Here, $Var(W) = \\sqrt{\\frac{2}{n_{in}+n_{out}}}$, where $n_{in}$은 이전 layer(input)의 노드 수\n",
    "\n",
    "### Bias 초기화\n",
    "가중치 초기화 뿐만 아니라 편향(bias) 초기값 또한 초기값 설정 또한 중요하다.\n",
    "보통의 경우에는 Bias는 0으로 초기화 하는 것이 일반적이다. ReLU의 경우 0.01과 같은 작은 값으로 b를 초기화 하는 것이 좋다는 보고도 있지만 모든 경우는 아니라 일반적으로는 0으로 초기화 하는 것이 효율적이다.\n",
    "\n",
    "----\n",
    "### Conclusion\n",
    "- Sigmoid, tanh 경우 Xavier 초기화 방법이 효율적이다.\n",
    "- ReLU계의 활성화 함수 사용 시 He 초기화 방법이 효율적이다.\n",
    "- **최근의 대부분의 모델에서는 He초기화를 주로 선택한다.**\n",
    "마지막으로, 대부분의 초기화 방법이 Normal Distribution과 Uniform Distribution을 따르는 두가지 방법이 있는데 이에대한 선택 기준에 대해서는 명확한 것이 없다. 하지만 He의 논문의 말을 인용하면,\n",
    "\n",
    "> 최근의 Deep CNN 모델들은 주로 Gaussian Distribution을 따르는 가중치 초기화 방법을 사용한다.\n",
    "\n",
    "따라서 Deep CNN의 경우 보통의 Gaussian 초기화 방법을 사용해 볼 수 있다.하지만 여러 초기화 방법들을 테스트하며 사용하는 것이 가장 좋은 방법일 것이다.\n",
    "\n",
    "출처 : https://reniew.github.io/13/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) 배치 정규화(Batch Normalization)\n",
    "\n",
    "### (1) 내부 공변량 변화(Internal Covariate Shift)\n",
    "\n",
    "### (2) 배치 정규화(Batch Normalization)\n",
    "> 한 번에 들어오는 배치 단위로 정규화\n",
    "\n",
    "https://shuuki4.wordpress.com/2016/01/13/batch-normalization-%EC%84%A4%EB%AA%85-%EB%B0%8F-%EA%B5%AC%ED%98%84/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
