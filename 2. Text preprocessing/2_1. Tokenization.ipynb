{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **텍스트 전처리** : 용도에 맞게 텍스트를 사전에 처리하는 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tokenization \n",
    "- <font color=red>**토큰화(Tokenization)**</font> : 코퍼스(corpus)에서 토큰(token)이라 불리는 단위로 나누는 작업\n",
    "- 코퍼스 데이터를 용도에 맞게 **토큰화(Tokenization), 정제(Cleaning), 정규화(Normalization)** 해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) 단어 토큰화(Word Tokenization)\n",
    "> 토큰의 기준을 단어(word)로 하는 경우\n",
    "\n",
    "- 단어 단위 외에도, 단어구, 의미를 갖는 문자열로 간주되기도 함\n",
    "- Examples : **Time is an illusion. Lunchtime double so!**\n",
    "    - 구두점(punctuation) : 온점(.), 컴마(,), 물음표, 세미콜론(;), 느낌표 등과 같은 기호들\n",
    "    - 구두점과 같은 문제를 제외시키는 간단한 단어 토큰화 : \"Time\", \"is\", \"an\", \"illusion\", \"Lunchtime\", \"double\", \"so\"\n",
    "    - 방법 : 구두점을 지운 뒤에, 공백을 기준으로 자름(split)\n",
    "    \n",
    "- 구두점이나 특수문자를 전부 제거하면 토크니 의미를 잃어버리는 경우도 발생함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 토큰화 중 생기는 선택의 순간\n",
    "- Example\n",
    "    - **Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.**\n",
    "    \n",
    "- 기존의 공개된 도구들을 사용했을 떄, 결과가 사용자의 목적과 일치한다면 해당 도구 사용--> <font color=red>NLTK</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T05:59:25.526626Z",
     "start_time": "2020-07-01T05:59:22.881661Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jnh78\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\jnh78\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T06:15:10.641801Z",
     "start_time": "2020-07-01T06:15:09.247530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(word_tokenize(\"\"\"Don't be fooled by the dark sounding name, \n",
    "                    Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T06:15:10.655767Z",
     "start_time": "2020-07-01T06:15:10.650779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "# 구두점을 별도로 분류하는 특징\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "print(WordPunctTokenizer().tokenize(\"\"\"Don't be fooled by the dark sounding name, \n",
    "                    Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T06:15:14.577275Z",
     "start_time": "2020-07-01T06:15:10.663745Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "# 케라스의 text_to_text_sequence는 모든 알파벳을 소문자로 바꾸고 구두점을 제거하지만 어포스트로피(')는 보존\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "print(text_to_word_sequence(\"\"\"Don't be fooled by the dark sounding name, \n",
    "Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) 토큰화에서 고려해야할 사항\n",
    "\n",
    "- 구두점이나 특수 문자를 단순 제외해선 안됌\n",
    "    - Ph.D, AT&T, 01/02/06, $45.55 등\n",
    "- 줄임말과 단어 내에 띄어쓰기가 있는 경우\n",
    "    - what are = what're, we are = we're\n",
    "    - New York, rock 'n' roll : 하나의 단어지만 중간에 띄어쓰기가 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 표준 토큰화 예제(아래 셀 참고)\n",
    "    - Penn Treebank Tokenization의 규칙\n",
    "        - 규칙1 : 하이픈(-)으로 구성된 단어는 하나로 유지\n",
    "        - 규칙2 : doesn't와 같이 어포스트로피(')로 '접어'가 함께하는 단어는 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T06:15:14.597224Z",
     "start_time": "2020-07-01T06:15:14.591241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "text=\"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) 문장 토큰화(Sentence Tokenization)\n",
    "> 온점(.)이 꼭 문장을 명확하게 구분해주진 못한다.\n",
    "\n",
    "\n",
    "EX1) IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서 ukairia777@gmail.com로 결과 좀 보내줘. 그러고나서 점심 먹으러 가자.\n",
    "\n",
    "EX2) Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.\n",
    "\n",
    "----------\n",
    "각 예제에서 첫번째 문장이 어디에서 끝나는지 확인하기 위해서 .을 기준으로 자르는 것은 부정확...  \n",
    "\n",
    "코퍼스가 어떤 국적의 언어인지, 해당 코퍼스 내에서 특수문자들이 어떻게 사용되고 있는지에 따라서 직접 규칙들을 정의해볼 수 있음 (100% 정확도는 어려움..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T06:15:14.619181Z",
     "start_time": "2020-07-01T06:15:14.610190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to mae sure no one was near.']\n"
     ]
    }
   ],
   "source": [
    "# NLTK에서 영어 문장의 토큰화를 수행하는 sent_tokenize를 지원\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text=\"\"\"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. \n",
    "Finally, the barber went up a mountain and almost to the edge of a cliff. \n",
    "He dug a hole in the midst of some reeds. He looked about, to mae sure no one was near.\"\"\"\n",
    "\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T06:15:15.289374Z",
     "start_time": "2020-07-01T06:15:15.285414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
     ]
    }
   ],
   "source": [
    "text=\"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **KSS(Korean Sentence Splitter)** : 박상길님이 개발한 한국어 문장 토큰화 도구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T06:15:18.360161Z",
     "start_time": "2020-07-01T06:15:16.116160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cython in c:\\users\\jnh78\\anaconda3\\envs\\tutorial\\lib\\site-packages (0.29.20)\n"
     ]
    }
   ],
   "source": [
    "!pip install cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T05:35:23.274256Z",
     "start_time": "2020-07-01T05:35:15.618731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kss\n",
      "  Downloading https://files.pythonhosted.org/packages/fc/bb/4772901b3b934ac204f32a0bd6fc0567871d8378f9bbc7dd5fd5e16c6ee7/kss-1.3.1.tar.gz\n",
      "Building wheels for collected packages: kss\n",
      "  Building wheel for kss (setup.py): started\n",
      "  Building wheel for kss (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for kss\n",
      "Failed to build kss\n",
      "Installing collected packages: kss\n",
      "    Running setup.py install for kss: started\n",
      "    Running setup.py install for kss: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\jnh78\\Anaconda3\\envs\\tutorial\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Public\\\\Documents\\\\ESTsoft\\\\CreatorTemp\\\\pip-install-wvd6lngp\\\\kss\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Public\\\\Documents\\\\ESTsoft\\\\CreatorTemp\\\\pip-install-wvd6lngp\\\\kss\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-wheel-56hbk2g3' --python-tag cp37\n",
      "       cwd: C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-install-wvd6lngp\\kss\\\n",
      "  Complete output (5 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_ext\n",
      "  building 'kss' extension\n",
      "  error: Microsoft Visual C++ 14.0 is required. Get it with \"Build Tools for Visual Studio\": https://visualstudio.microsoft.com/downloads/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for kss\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\jnh78\\Anaconda3\\envs\\tutorial\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Public\\\\Documents\\\\ESTsoft\\\\CreatorTemp\\\\pip-install-wvd6lngp\\\\kss\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Public\\\\Documents\\\\ESTsoft\\\\CreatorTemp\\\\pip-install-wvd6lngp\\\\kss\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-record-fbbgwlfi\\install-record.txt' --single-version-externally-managed --compile\n",
      "         cwd: C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-install-wvd6lngp\\kss\\\n",
      "    Complete output (5 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_ext\n",
      "    building 'kss' extension\n",
      "    error: Microsoft Visual C++ 14.0 is required. Get it with \"Build Tools for Visual Studio\": https://visualstudio.microsoft.com/downloads/\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\Users\\jnh78\\Anaconda3\\envs\\tutorial\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Public\\\\Documents\\\\ESTsoft\\\\CreatorTemp\\\\pip-install-wvd6lngp\\\\kss\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Public\\\\Documents\\\\ESTsoft\\\\CreatorTemp\\\\pip-install-wvd6lngp\\\\kss\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-record-fbbgwlfi\\install-record.txt' --single-version-externally-managed --compile Check the logs for full command output.\n"
     ]
    }
   ],
   "source": [
    "!pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T06:15:30.090064Z",
     "start_time": "2020-07-01T06:15:28.519265Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-de5123a8d97b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m text='''딥 러닝 자연어 처리가 재미있기는 합니다. \n\u001b[0;32m      4\u001b[0m \u001b[0m그런데\u001b[0m \u001b[0m문제는\u001b[0m \u001b[0m영어보다\u001b[0m \u001b[0m한국어로\u001b[0m \u001b[0m할\u001b[0m \u001b[0m때\u001b[0m \u001b[0m너무\u001b[0m \u001b[0m어려워요\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m 농담아니에요. 이제 해보면 알걸요?'''\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'kss'"
     ]
    }
   ],
   "source": [
    "import kss\n",
    "\n",
    "text='''딥 러닝 자연어 처리가 재미있기는 합니다. \n",
    "그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. \n",
    "농담아니에요. 이제 해보면 알걸요?'''\n",
    "print(kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) 이진분류기\n",
    "> 문장 토큰화에서의 예외 사항을 발생시키는 온점 처리를 위해서 입력에 따라 두 개의 클래스로 분류하는 이진 분류기(binary classifier)를 사용\n",
    "\n",
    "- 두 개의 클래스?\n",
    "    - 1) 온점이 약어(abbreviation)으로 쓰이는 겨우\n",
    "    - 2) 온점이 정말로 문장의 구분자일 경우\n",
    "    \n",
    "- 이진 분류기는 임의로 정한 여러가지 규칙을 코딩한 함수일 수도 있는데 머신러닝을 통해 이진 분류기를 구현하기도 함.\n",
    "- 온점이 어떤 클래스에 속하는지 알기 위해서는 어떤 온점이 주로 약어로 쓰이는지 알아야 함 --> <font color=red>**약어사전**</font> 중요 \n",
    "- 영어권의 경우, [Oxford English Dictionary](https://public.oed.com/how-to-use-the-oed/abbreviations/) 참조\n",
    "- 문장 토큰화 규칙을 짤 때, 발생할 수 있는 여러가지 예외사항을 다룬 참고자료를 보고 싶으면 [이곳](https://tech.grammarly.com/blog/posts/How-to-Split-Sentences.html) 참조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) 한국어 토큰화 어려움\n",
    "- 한국어는 교착어 --> 형태소(morpheme)를 반드시 이해해야함...\n",
    "- 한국어는 띄어쓰기가 영어보다 잘 지켜지지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) 품사 태깅(Part-of-speech tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) NLTK와 KoNLPy를 이용한 영어, 한국어 토큰화 실습\n",
    "- NLTK\n",
    "    - Penn Treebank POS Tags라는 기준 적용\n",
    "    \n",
    "- KoNLPy\n",
    "    - KoNLPy를 통해서 사용할 수 있는 형태소 분석기들 : **Okt(Open Korea Text), 메캅(Mecab), 코모란(Komoran), 한나눔(Hannanum), 꼬꼬마(Kkma)**\n",
    "    - 한국어 NLP에서 형태소 분석기 사용 --> 단어 토큰화가 아니라 **형태소 토큰화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T06:21:49.304820Z",
     "start_time": "2020-07-01T06:21:48.067131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n",
      "\n",
      "\n",
      "[('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text=\"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
    "print(word_tokenize(text))\n",
    "print('\\n')\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "x=word_tokenize(text)\n",
    "print(pos_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T06:24:02.296548Z",
     "start_time": "2020-07-01T06:24:02.266629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n",
      "[('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')] \n",
      "\n",
      "['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt=Okt()\n",
    "# 형태소 추출\n",
    "print(okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "\n",
    "# pos : 품사 태깅\n",
    "print(okt.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"), '\\n')\n",
    "\n",
    "# 명사 추출\n",
    "print(okt.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T06:24:42.608074Z",
     "start_time": "2020-07-01T06:24:29.100198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n",
      "[('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n",
      "['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "\n",
    "kkma=Kkma()\n",
    "print(kkma.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "print(kkma.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "print(kkma.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okt 형태소 분석기와 Kkma 형태소 분석기의 결과가 다름...  \n",
    "$\\Longrightarrow$ **필요 용도에 어떤 형태소 분석기가 가장 적절한지 판단하고 사용하면 됌**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
